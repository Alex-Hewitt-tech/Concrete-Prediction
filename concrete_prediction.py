# -*- coding: utf-8 -*-
"""Concrete_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iIH6iEAc95iIYGIzwvmxfA6ofc_d_LpH
"""

#Packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
import tensorflow as tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.utils import to_categorical

#Sets seed
np.random.seed(42)
tensorflow.random.set_seed(42)

#Reads in concrete.csv
concrete = pd.read_csv("concrete.csv")

#Gives information about the data we are working with
concrete.info()

#Shows the first 5 rows to see what our data looks like
concrete.head()

#Function to create a categorical variable using the strength variable to classify strength into 3 levels of 'low','medium','high' based on MPA ranges of below 20 are low, between 20 and 40
#is medium and above 40 is high.
def strength_level(strength):
    if strength <= 20:
       return 'low'
    elif 20 < strength <= 40:
       return 'medium'
    else:
       return 'high'

#Applies the function to the strength column to make the new strength_level coloumn
concrete['strength_level'] = concrete['strength'].apply(strength_level)

#Collects the numerical variables for the prediction in concrete_x and the variable we are predicting in concrete_y
concrete_x = concrete.drop(columns = ['strength_level','strength'])
concrete_y = concrete['strength_level']

#Classifies label enconder to turn the strength_level variable into numerical variable,then implemented One-hot Encoding, and finally splits the
#data 70% training and 30% test
label_encode = LabelEncoder()
concrete_y_encode = label_encode.fit_transform(concrete_y)
concrete_y_one = to_categorical(concrete_y_encode)
concrete_x_train, concrete_x_test, concrete_y_train, concrete_y_test = train_test_split(concrete_x,concrete_y_one,test_size = 0.3,random_state = 42)

#Creates neural network model(Baseline Model)
concrete_model = Sequential()

#Input layer with 128 units using relu activation
concrete_model.add(Dense(units = 128, activation = 'relu', input_shape = (concrete_x.shape[1],)))

#Hidden layer 1 with 64 units using relu activation
concrete_model.add(Dense(units = 64, activation = 'relu'))

#Hidden layer2 with 32 units using relu activation
concrete_model.add(Dense(units = 32, activation = 'relu'))

#Output layer with 3 units for each class we want to predict using softmax activation
concrete_model.add(Dense(units = 3, activation = 'softmax'))

#Learing process defined here with categorical_crossentropy, Adam optimizer(for backprpagation), and accuracy metric
concrete_model.compile(
  loss = 'categorical_crossentropy',
  optimizer = Adam(learning_rate = 0.001),
  metrics = ['accuracy']
)

#Fits model with training data for the baseline model
history = concrete_model.fit(x = concrete_x_train,y = concrete_y_train,verbose = False)

#Evaluates baseline model
concrete_model.evaluate(concrete_x_test, concrete_y_test)

#################################################################################################################################################################################################

#Used boxplot to see if many variables had outliers(interchanged variables)
plt.boxplot(concrete['age'], vert=False)

#Sets seed
np.random.seed(42)

#Creates a duplicate of concrete data stored in concrete2
concrete2 = concrete

#Gets rid of all outliers using mean and times 2 standard deviation from all numerical variables but strength and strength_level
for column in concrete2.columns:
    if column == 'strength' or column == 'strength_level':
       continue
    mean = concrete2[column].mean()
    std = concrete2[column].std()
    lower_bound = mean - std * 2
    upper_bound = mean + std * 2
    concrete2 = concrete2[(concrete2[column] >= lower_bound) & (concrete2[column] <= upper_bound)]

#Collects the numerical variables for the prediction in concrete_x2 and normalizes the data using the MinMaxScaler
concrete_x2 = concrete2.drop(columns = ['strength_level','strength'])
concrete_x2_columns = concrete_x2.columns
normalize_scaler = MinMaxScaler()
concrete_x2[concrete_x2_columns] = normalize_scaler.fit_transform(concrete_x2[concrete_x2_columns])

#Collects the strength_level variable we are trying to predict
concrete_y2 = concrete2['strength_level']

#Classifies label enconder to turn the strength_level variable into numerical variable,then implemented One-hot Encoding
label_encode = LabelEncoder()
concrete_y_encode2 = label_encode.fit_transform(concrete_y2)
concrete_y_one2 = to_categorical(concrete_y_encode2)

#splits the data 70% training and 30% test for the other model
concrete_x_train2, concrete_x_test2, concrete_y_train2, concrete_y_test2 = train_test_split(concrete_x2,concrete_y_one2,test_size = 0.3,random_state = 42)

#Creates neural network model
concrete_model2 = Sequential()

#Input layer with 256 units using relu activation, with most layers having Dropout
concrete_model2.add(Dense(units = 256, activation = 'relu', input_shape = (concrete_x.shape[1],)))
concrete_model2.add(Dropout(rate = 0.4))

#Hidden layer 1 with 128 units using relu activation
concrete_model2.add(Dense(units = 128, activation = 'relu'))
concrete_model2.add(Dropout(rate = 0.3))

#Hidden layer2 with 64 units using relu activation
concrete_model2.add(Dense(units = 64, activation = 'relu'))
concrete_model2.add(Dropout(rate = 0.2))

#Hidden layer3 with 32 units using relu activation
concrete_model2.add(Dense(units = 32, activation = 'relu'))
concrete_model2.add(Dropout(rate = 0.1))

#Output layer with 3 units for each class we want to predict using softmax activation
concrete_model2.add(Dense(units = 3, activation = 'softmax'))

#Learning process defined here with categorical_crossentropy, Adam optimizer(for backprpagation), and accuracy metric
concrete_model2.compile(
  loss = 'categorical_crossentropy',
  optimizer = Adam(learning_rate = 0.001),
  metrics = ['accuracy']
)
#Callbacks defined here to stop training if their is no improvements and learning rate reducer if there are no improvements
early_call = EarlyStopping(patience = 10)
reduce_lr = ReduceLROnPlateau(factor = 0.3,patience = 5)
#Fits training data with concrete_model
history = concrete_model2.fit(x = concrete_x_train2,y = concrete_y_train2,epochs = 35,batch_size = 16, validation_split = 0.2,
                             callbacks = [early_call,reduce_lr] ,verbose = False)

#Evaluates neural network concrete model
concrete_model2.evaluate(concrete_x_test2, concrete_y_test2)

#Classifcation report having values precision,recall,f1-score,support
y_pred_prob = concrete_model2.predict(concrete_x_test2)
y_pred = np.argmax(y_pred_prob, axis=1)
class_report = classification_report(np.argmax(concrete_y_test2, axis=1), y_pred, target_names=label_encode.classes_)
print(class_report)

#The classifcation report shows good values for each high,low,medium for all the values in the report such as precison,recall,f1-score,etc. Showing that the model is running well.

#Creates confusion matrix to see where the predictive values went and if they are correct
cm = confusion_matrix(np.argmax(concrete_y_test2, axis=1), y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encode.classes_, yticklabels=label_encode.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

#Shows that our model runs very well with majority True Positive predictions of 69 for high,46 for low,85 for medium and the highest false positive only being 14 where medium was accidentally picked 14 times instead of the actual
#label of high.np

#Plots training loss and validation loss to see if overfitting or underfitting has occured and the plot shows that neither are occuring.
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#The graph above shows that our model fits very well with training loss and validation loss constantly overlapping and validation loss gradually decreases.